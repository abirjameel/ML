{"cells":[{"cell_type":"markdown","source":["### Data Preprocessing using Spark \nthe link to download the data \nhttp://files.grouplens.org/datasets/movielens/ml-100k.zip"],"metadata":{}},{"cell_type":"markdown","source":["#### Load the Dataset which contains the information about users, later we will load other datasets that contains the description about movie and ratings."],"metadata":{}},{"cell_type":"code","source":["## read the file by changing the path \nuser_data = sc.textFile(\"/FileStore/tables/kv4939va1496946928579/u.user\")\nuser_data.first()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["user_fields = user_data.map(lambda line: line.split(\"\\t\"))\nuser_fields.first()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Get the information about Users\nSuch as Total number of Users\nSince occupation is an class varible we need to find number distinct classes \nCount of each Genders \nand Distinct Count of Zip-codes"],"metadata":{}},{"cell_type":"code","source":["## Counting the Number of Users \nnum_users = user_fields.map(lambda fields: fields[0]).count()\n\nnum_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()\n\nnum_genders = user_fields.map(lambda fields:fields[2]).distinct().count()\n\nnum_zipcodes = user_fields.map(lambda fields:fields[4]).distinct().count()\n\nprint \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nages = user_fields.map(lambda x: int(x[1])).collect()\nplt.hist(ages, bins=20, color='red', normed=True)\nfig = plt.gcf()\nfig.set_size_inches(16, 10)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import numpy as np \ncount_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()\nx_axis1 = np.array([c[0] for c in count_by_occupation])\ny_axis1 = np.array([c[1] for c in count_by_occupation])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["pos = np.arange(len(x_axis)) \nwidth = 1.0\nax = plt.axes()\nax.set_xticks(pos + (width/2))\nax.set_xticklabels(x_axis)\nplt.bar(pos, y_axis, width, color='lightblue')\nplt.xticks(rotation=90)\nfig = plt.gcf()\nfig.set_size_inches(16, 10)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()\ncount_by_occupation2"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["movie_data = sc.textFile(\"/FileStore/tables/djtncxwv1496949426397/u.item\")\nprint movie_data.first()\nnum_movies = movie_data.count()\nprint \"Movies: %d\" % num_movies\n# movie_data.take(1)[0][2]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["Movie_feat = movie_data.map(lambda line: line.split(\"|\"))\nMovie_feat.take(3)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["def convert_year(x):\n  try:\n    return int(x[-4:])\n  except:\n    return 1900 # there is a 'bad' data point with a blank year, which we set to 1900 and will filter out later"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["movie_fields = movie_data.map(lambda lines: lines.split(\"|\"))\nyears = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["years_filtered = years.filter(lambda x: x != 1900)\nyears_filtered.take(1)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nmovie_ages = years_filtered.map(lambda yr: 2017 - yr).countByValue()\nvalues = movie_ages.values()\nbins = movie_ages.keys()\nplt.hist(values, bins=bins, color='lightblue', normed = True)\nfig1 = plt.gcf()\nfig1.set_size_inches(16,10)\ndisplay(fig1)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["rating_data_raw = sc.textFile(\"/FileStore/tables/y3dmjo4l1497452649183/u.data\")\nprint rating_data_raw.first()\nnum_ratings = rating_data_raw.count()\nprint \"Ratings: %d\" % num_ratings\nrating_data = rating_data_raw.map(lambda line: line.split(\"\\t\"))\nrating_data.take(2)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["#### Calculating Stats of ratings"],"metadata":{}},{"cell_type":"code","source":["import numpy as np \nratings = rating_data.map(lambda fields: int(fields[2]))\nmax_rating = ratings.reduce(lambda x, y: max(x, y))\nmin_rating = ratings.reduce(lambda x, y: min(x,y))\nmean_rating = ratings.reduce(lambda x, y: (x + y))/ num_ratings\nmedian_rating = np.median(ratings.collect())"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Display the ratings basic stats"],"metadata":{}},{"cell_type":"code","source":["print \"Min rating: %d\" % min_rating\nprint \"Max rating: %d\" % max_rating\nprint \"Average rating: %2.4f\" % mean_rating\nprint \"Median rating: %d\" % median_rating"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### Now we have to calculate number of ratings per user and number of ratings per movie i.e. number of total ratings divided by num of users.\nwe see that on an average each user has given 106 ratings and each movie receives approx. 59 ratings"],"metadata":{}},{"cell_type":"code","source":["ratings_per_user = num_ratings / num_users\nratings_per_movie = num_ratings / num_movies\nprint \"Average # of ratings per user: %2.2f\" % ratings_per_user\nprint \"Average # of ratings per movie: %2.2f\" % ratings_per_movie"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### Statistics from Spark inbuilt stats function"],"metadata":{}},{"cell_type":"code","source":["ratings.stats()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["To compute the distribution of ratings per user, we will first extract the user ID as \nkey and rating as value from\nrating_data RDD. We will then group the ratings by\nuser ID using Spark's\ngroupByKey function:"],"metadata":{}},{"cell_type":"code","source":["### lets have a look at the ratings data get the fields \nrating_data.take(5)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]),int(fields[2]))).groupByKey()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["## for each key (user ID), we will find the size of the set of ratings; this will give us the number of ratings for that user:\nuser_ratings_byuser = user_ratings_grouped.map(lambda (k, v): (k,len(v)))\nuser_ratings_byuser.take(3)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### plot the histogram of number of ratings per user using our favorite hist function"],"metadata":{}},{"cell_type":"code","source":["user_ratings_byuser_local = user_ratings_byuser.map(lambda (k, v): v).collect()\nplt.hist(user_ratings_byuser_local, bins=200, color='lightblue', normed=True)\nplt.subplot(211)\n#fig3.set_size_inches(16,10)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Plotting number of ratings given to each movie"],"metadata":{}},{"cell_type":"code","source":["movie_ratings_grouped = rating_data.map(lambda fields: (int(fields[1]),int(fields[2]))).groupByKey()\n## for each key (user ID), we will find the size of the set of ratings; this will give us the number of ratings for that user:\nratings_bymovie = movie_ratings_grouped.map(lambda (k, v): (k,len(v)))\nratings_bymovie.take(3)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["ratings_bymovie_local = ratings_bymovie.map(lambda (k, v): v).collect()\nplt.hist(ratings_bymovie_local, bins=200, color='cyan', normed=True)\nplt.subplot(211)\n#fig3.set_size_inches(16,10)\n#display()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["def extract_datetime(ts):\n    import datetime\n    return datetime.datetime.fromtimestamp(ts)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["timestamps = rating_data.map(lambda fields: int(fields[3]))\nhour_of_day = timestamps.map(lambda ts: extract_datetime(ts).hour)\nhour_of_day.take(5)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Converting Time Stamp to Categorical Features"],"metadata":{}},{"cell_type":"code","source":["def assign_tod(hr):\n  times_of_day = {\n    'morning' : range(7, 12),\n    'lunch' : range(12, 14),\n    'afternoon' : range(14, 18),\n    'evening' : range(18, 23),\n    'night' : range(24, 7),\n    'Super night': range(0,7)\n  }\n  for k, v in times_of_day.iteritems():\n    if hr in v:\n      return k"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["time_of_day = hour_of_day.map(lambda hr: assign_tod(hr))\ntime_of_day.take(5)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### Missing Data Treatment"],"metadata":{}},{"cell_type":"code","source":["years_pre_processed = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)).collect()\nyears_pre_processed_array = np.array(years_pre_processed)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["years_pre_processed_array"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["#### we will compute the mean and median year"],"metadata":{}},{"cell_type":"code","source":["mean_year = np.mean(years_pre_processed_array[years_pre_processed_array!=1900])\nmedian_year = np.median(years_pre_processed_array[years_pre_processed_array!=1900])"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# Lets have a look at mean year\nmean_year"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Lets have a look at median year\nmedian_year"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["index_bad_data = np.where(years_pre_processed_array==1900)[0][0]\nyears_pre_processed_array[index_bad_data] = median_year"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["### Extracting features from data\nCategorical Features"],"metadata":{}},{"cell_type":"code","source":["## occupation variable\nall_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()\nall_occupations.sort()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["all_occupations"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\ndf = spark.createDataFrame(zip(range(len(all_occupations)), all_occupations), [\"id\", \"category\"])\nstringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nmodel = stringIndexer.fit(df)\ndf.show(5)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["indexed = model.transform(df)\nencoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\nencoded = encoder.transform(indexed)\nencoded.show()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":54}],"metadata":{"name":"Data Preprocessing using Spark","notebookId":1230058994194660},"nbformat":4,"nbformat_minor":0}
